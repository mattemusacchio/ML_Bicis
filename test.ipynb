{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROYECTO FINAL - MACHINE LEARNING \n",
    "# PREDICCI√ìN DE ARRIBOS DE BICICLETAS P√öBLICAS GCBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Para el modelado\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 1: CARGA Y EXPLORACI√ìN INICIAL DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö¥ CARGANDO DATOS DE BICICLETAS P√öBLICAS BA...\n",
      "üìä Datos cargados:\n",
      "   - Trips: 3,559,284 registros, 17 columnas\n",
      "   - Usuarios: 197,077 registros, 5 columnas\n"
     ]
    }
   ],
   "source": [
    "print(\"üö¥ CARGANDO DATOS DE BICICLETAS P√öBLICAS BA...\")\n",
    "\n",
    "# Cargar datos de trips 2024\n",
    "trips_df = pd.read_csv('data/raw/trips_2024.csv')\n",
    "usuarios_df = pd.read_csv('data/raw/usuarios_ecobici_2024.csv')\n",
    "\n",
    "print(f\"üìä Datos cargados:\")\n",
    "print(f\"   - Trips: {trips_df.shape[0]:,} registros, {trips_df.shape[1]} columnas\")\n",
    "print(f\"   - Usuarios: {usuarios_df.shape[0]:,} registros, {usuarios_df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç EXPLORANDO ESTRUCTURA DE TRIPS...\n",
      "Columnas de trips: ['id_recorrido', 'duracion_recorrido', 'fecha_origen_recorrido', 'id_estacion_origen', 'nombre_estacion_origen', 'direccion_estacion_origen', 'long_estacion_origen', 'lat_estacion_origen', 'fecha_destino_recorrido', 'id_estacion_destino', 'nombre_estacion_destino', 'direccion_estacion_destino', 'long_estacion_destino', 'lat_estacion_destino', 'id_usuario', 'modelo_bicicleta', 'genero']\n",
      "\n",
      "Primeras filas:\n",
      "   id_recorrido  duracion_recorrido fecha_origen_recorrido  \\\n",
      "0      20428222                 568    2024-01-23 18:36:00   \n",
      "1      20431744                1355    2024-01-23 22:41:20   \n",
      "2      20429936                   0    2024-01-23 20:06:22   \n",
      "3      20429976                   0    2024-01-23 20:08:17   \n",
      "4      20424802                 680    2024-01-23 15:18:39   \n",
      "\n",
      "   id_estacion_origen  nombre_estacion_origen direccion_estacion_origen  \\\n",
      "0                 513     308 - SAN MARTIN II       Av. San Mart√≠n 5129   \n",
      "1                 460  133 - BEIRO Y SEGUROLA             Segurola 3194   \n",
      "2                 467      328 - SARMIENTO II            Sarmiento 2037   \n",
      "3                 382          204 - Biarritz             Biarritz 2403   \n",
      "4                 137  137 - AZOPARDO Y CHILE              AZOPARDO 700   \n",
      "\n",
      "   long_estacion_origen  lat_estacion_origen fecha_destino_recorrido  \\\n",
      "0            -58.490739           -34.597130     2024-01-23 18:45:28   \n",
      "1            -58.511930           -34.607500     2024-01-23 23:03:55   \n",
      "2            -58.395893           -34.605514     2024-01-23 20:06:22   \n",
      "3            -58.477255           -34.605431     2024-01-23 20:08:17   \n",
      "4            -58.367492           -34.615598     2024-01-23 15:29:59   \n",
      "\n",
      "   id_estacion_destino nombre_estacion_destino  \\\n",
      "0                  498            055 - HABANA   \n",
      "1                  382          204 - Biarritz   \n",
      "2                    6     006 - Parque Lezama   \n",
      "3                  460  133 - BEIRO Y SEGUROLA   \n",
      "4                  150     150 - RODRIGO BUENO   \n",
      "\n",
      "                    direccion_estacion_destino  long_estacion_destino  \\\n",
      "0  Gral. Jos√© Gervasio Artigas 4298 (y Habana)             -58.494959   \n",
      "1                                Biarritz 2403             -58.477255   \n",
      "2                   Avenida Martin Garcia, 295             -58.369758   \n",
      "3                                Segurola 3194             -58.511930   \n",
      "4                              Av. Espa√±a 2200             -58.355465   \n",
      "\n",
      "   lat_estacion_destino  id_usuario modelo_bicicleta  genero  \n",
      "0            -34.586598    992557.0              FIT    MALE  \n",
      "1            -34.605431    320782.0              FIT  FEMALE  \n",
      "2            -34.628526    828678.0              FIT  FEMALE  \n",
      "3            -34.607500    320782.0           ICONIC  FEMALE  \n",
      "4            -34.618755    861425.0              FIT  FEMALE  \n",
      "\n",
      "Info de trips:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3559284 entries, 0 to 3559283\n",
      "Data columns (total 17 columns):\n",
      " #   Column                      Dtype  \n",
      "---  ------                      -----  \n",
      " 0   id_recorrido                int64  \n",
      " 1   duracion_recorrido          int64  \n",
      " 2   fecha_origen_recorrido      object \n",
      " 3   id_estacion_origen          int64  \n",
      " 4   nombre_estacion_origen      object \n",
      " 5   direccion_estacion_origen   object \n",
      " 6   long_estacion_origen        float64\n",
      " 7   lat_estacion_origen         float64\n",
      " 8   fecha_destino_recorrido     object \n",
      " 9   id_estacion_destino         int64  \n",
      " 10  nombre_estacion_destino     object \n",
      " 11  direccion_estacion_destino  object \n",
      " 12  long_estacion_destino       float64\n",
      " 13  lat_estacion_destino        float64\n",
      " 14  id_usuario                  float64\n",
      " 15  modelo_bicicleta            object \n",
      " 16  genero                      object \n",
      "dtypes: float64(5), int64(4), object(8)\n",
      "memory usage: 461.6+ MB\n",
      "None\n",
      "\n",
      "üîç EXPLORANDO ESTRUCTURA DE USUARIOS...\n",
      "Columnas de usuarios: ['id_usuario', 'genero_usuario', 'edad_usuario', 'fecha_alta', 'hora_alta']\n",
      "\n",
      "Primeras filas:\n",
      "   id_usuario genero_usuario  edad_usuario  fecha_alta hora_alta\n",
      "0     1094487          OTHER            35  2024-01-18  09:21:02\n",
      "1     1097128         FEMALE            21  2024-01-21  23:02:26\n",
      "2     1102901          OTHER            19  2024-01-29  19:33:32\n",
      "3     1096100         FEMALE            29  2024-01-20  18:57:05\n",
      "4     1103049           MALE            21  2024-01-30  00:49:33\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüîç EXPLORANDO ESTRUCTURA DE TRIPS...\")\n",
    "print(f\"Columnas de trips: {list(trips_df.columns)}\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "print(trips_df.head())\n",
    "\n",
    "print(f\"\\nInfo de trips:\")\n",
    "print(trips_df.info())\n",
    "\n",
    "print(\"\\nüîç EXPLORANDO ESTRUCTURA DE USUARIOS...\")\n",
    "print(f\"Columnas de usuarios: {list(usuarios_df.columns)}\")\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "print(usuarios_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 2: PREPROCESAMIENTO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõ†Ô∏è PREPROCESANDO DATOS...\n",
      "üìÖ Datos filtrados hasta agosto 2024: 2,155,229 registros\n",
      "   - Fecha m√≠nima: 2024-01-01 00:06:50\n",
      "   - Fecha m√°xima: 2024-08-30 23:57:59\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüõ†Ô∏è PREPROCESANDO DATOS...\")\n",
    "\n",
    "# Convertir fechas a datetime\n",
    "trips_df['fecha_origen_recorrido'] = pd.to_datetime(trips_df['fecha_origen_recorrido'])\n",
    "trips_df['fecha_destino_recorrido'] = pd.to_datetime(trips_df['fecha_destino_recorrido'])\n",
    "\n",
    "# Filtrar solo datos hasta agosto 2024 como especifica el enunciado\n",
    "trips_df = trips_df[trips_df['fecha_origen_recorrido'] <= '2024-08-31']\n",
    "\n",
    "print(f\"üìÖ Datos filtrados hasta agosto 2024: {trips_df.shape[0]:,} registros\")\n",
    "\n",
    "# Verificar rango de fechas\n",
    "print(f\"   - Fecha m√≠nima: {trips_df['fecha_origen_recorrido'].min()}\")\n",
    "print(f\"   - Fecha m√°xima: {trips_df['fecha_origen_recorrido'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà AN√ÅLISIS EXPLORATORIO...\n",
      "Duraci√≥n promedio de viajes: 1303.73 segundos\n",
      "Duraci√≥n mediana: 872.00 segundos\n",
      "Estaciones origen √∫nicas: 374\n",
      "Estaciones destino √∫nicas: 376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüìà AN√ÅLISIS EXPLORATORIO...\")\n",
    "\n",
    "# Estad√≠sticas b√°sicas de duraci√≥n\n",
    "print(f\"Duraci√≥n promedio de viajes: {trips_df['duracion_recorrido'].mean():.2f} segundos\")\n",
    "print(f\"Duraci√≥n mediana: {trips_df['duracion_recorrido'].median():.2f} segundos\")\n",
    "\n",
    "# Cantidad de estaciones √∫nicas\n",
    "n_estaciones_origen = trips_df['id_estacion_origen'].nunique()\n",
    "n_estaciones_destino = trips_df['id_estacion_destino'].nunique()\n",
    "print(f\"Estaciones origen √∫nicas: {n_estaciones_origen}\")\n",
    "print(f\"Estaciones destino √∫nicas: {n_estaciones_destino}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 3: FEATURE ENGINEERING - CREACI√ìN DE VENTANAS TEMPORALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è CREANDO FEATURES TEMPORALES...\n",
      "‚úÖ Features temporales creadas con ventanas de 30 minutos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n‚öôÔ∏è CREANDO FEATURES TEMPORALES...\")\n",
    "\n",
    "# Definir delta T (usaremos 30 minutos como ejemplo)\n",
    "DELTA_T_MINUTES = 30\n",
    "\n",
    "# Agregar features temporales\n",
    "trips_df['a√±o'] = trips_df['fecha_origen_recorrido'].dt.year\n",
    "trips_df['mes'] = trips_df['fecha_origen_recorrido'].dt.month\n",
    "trips_df['dia'] = trips_df['fecha_origen_recorrido'].dt.day\n",
    "trips_df['hora'] = trips_df['fecha_origen_recorrido'].dt.hour\n",
    "trips_df['dia_semana'] = trips_df['fecha_origen_recorrido'].dt.dayofweek\n",
    "trips_df['es_fin_de_semana'] = trips_df['dia_semana'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Crear ventanas temporales de 30 minutos\n",
    "trips_df['timestamp_rounded'] = trips_df['fecha_origen_recorrido'].dt.floor(f'{DELTA_T_MINUTES}min')\n",
    "\n",
    "print(f\"‚úÖ Features temporales creadas con ventanas de {DELTA_T_MINUTES} minutos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar trips_df \n",
    "trips_df.to_csv('data/processed/trips_2024_preprocessed.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 4: CREACI√ìN DEL DATASET PARA ML - AGREGACI√ìN POR VENTANAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ CREANDO DATASET PARA MACHINE LEARNING...\n",
      "üìç Total de estaciones identificadas: 376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüéØ CREANDO DATASET PARA MACHINE LEARNING...\")\n",
    "\n",
    "# Obtener lista de todas las estaciones\n",
    "todas_las_estaciones = pd.concat([\n",
    "    trips_df[['id_estacion_origen', 'nombre_estacion_origen', 'lat_estacion_origen', 'long_estacion_origen']].rename(columns={\n",
    "        'id_estacion_origen': 'id_estacion',\n",
    "        'nombre_estacion_origen': 'nombre_estacion', \n",
    "        'lat_estacion_origen': 'lat_estacion',\n",
    "        'long_estacion_origen': 'long_estacion'\n",
    "    }),\n",
    "    trips_df[['id_estacion_destino', 'nombre_estacion_destino', 'lat_estacion_destino', 'long_estacion_destino']].rename(columns={\n",
    "        'id_estacion_destino': 'id_estacion',\n",
    "        'nombre_estacion_destino': 'nombre_estacion',\n",
    "        'lat_estacion_destino': 'lat_estacion', \n",
    "        'long_estacion_destino': 'long_estacion'\n",
    "    })\n",
    "]).drop_duplicates(subset=['id_estacion'])\n",
    "\n",
    "print(f\"üìç Total de estaciones identificadas: {len(todas_las_estaciones)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì§ AGREGANDO PARTIDAS POR VENTANA TEMPORAL...\n",
      "‚úÖ Partidas agregadas: 1146667 registros\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüì§ AGREGANDO PARTIDAS POR VENTANA TEMPORAL...\")\n",
    "\n",
    "# Contar partidas por estaci√≥n y ventana temporal\n",
    "partidas_por_ventana = (trips_df.groupby(['timestamp_rounded', 'id_estacion_origen'])\n",
    "                       .agg({\n",
    "                           'id_recorrido': 'count',\n",
    "                           'hora': 'first',\n",
    "                           'dia_semana': 'first', \n",
    "                           'es_fin_de_semana': 'first',\n",
    "                           'mes': 'first',\n",
    "                           'dia': 'first'\n",
    "                       })\n",
    "                       .rename(columns={'id_recorrido': 'partidas'})\n",
    "                       .reset_index())\n",
    "\n",
    "print(f\"‚úÖ Partidas agregadas: {len(partidas_por_ventana)} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• AGREGANDO ARRIBOS POR VENTANA TEMPORAL...\n",
      "‚úÖ Arribos agregados: 1204063 registros\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüì• AGREGANDO ARRIBOS POR VENTANA TEMPORAL...\")\n",
    "\n",
    "# Para los arribos, necesitamos usar la fecha de destino\n",
    "trips_df['timestamp_destino_rounded'] = trips_df['fecha_destino_recorrido'].dt.floor(f'{DELTA_T_MINUTES}min')\n",
    "\n",
    "# Contar arribos por estaci√≥n y ventana temporal\n",
    "arribos_por_ventana = (trips_df.groupby(['timestamp_destino_rounded', 'id_estacion_destino'])\n",
    "                      .agg({\n",
    "                          'id_recorrido': 'count'\n",
    "                      })\n",
    "                      .rename(columns={'id_recorrido': 'arribos'})\n",
    "                      .reset_index()\n",
    "                      .rename(columns={'timestamp_destino_rounded': 'timestamp_rounded',\n",
    "                                     'id_estacion_destino': 'id_estacion'}))\n",
    "\n",
    "print(f\"‚úÖ Arribos agregados: {len(arribos_por_ventana)} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 5: CREACI√ìN DE FEATURES Y TARGETS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ CREANDO FEATURES (X) Y TARGETS (Y)...\n",
      "üìä Dataset base creado: 4385664 registros\n",
      "   - Ventanas temporales: 11664\n",
      "   - Estaciones: 376\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüéØ CREANDO FEATURES (X) Y TARGETS (Y)...\")\n",
    "\n",
    "# Obtener todas las combinaciones de timestamp y estaci√≥n posibles\n",
    "timestamps_unicos = pd.date_range(\n",
    "    start=trips_df['timestamp_rounded'].min(),\n",
    "    end=trips_df['timestamp_rounded'].max(), \n",
    "    freq=f'{DELTA_T_MINUTES}min'\n",
    ")\n",
    "\n",
    "# Crear un DataFrame base con todas las combinaciones\n",
    "base_df = pd.DataFrame([\n",
    "    (ts, estacion) \n",
    "    for ts in timestamps_unicos \n",
    "    for estacion in todas_las_estaciones['id_estacion'].unique()\n",
    "], columns=['timestamp', 'id_estacion'])\n",
    "\n",
    "print(f\"üìä Dataset base creado: {len(base_df)} registros\")\n",
    "print(f\"   - Ventanas temporales: {len(timestamps_unicos)}\")\n",
    "print(f\"   - Estaciones: {todas_las_estaciones['id_estacion'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó COMBINANDO PARTIDAS Y ARRIBOS...\n",
      "‚úÖ Dataset combinado: 4385664 registros\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüîó COMBINANDO PARTIDAS Y ARRIBOS...\")\n",
    "\n",
    "# Merge con partidas\n",
    "dataset = base_df.merge(\n",
    "    partidas_por_ventana.rename(columns={'timestamp_rounded': 'timestamp', 'id_estacion_origen': 'id_estacion'}),\n",
    "    on=['timestamp', 'id_estacion'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge con arribos (estos ser√°n nuestros targets futuros)\n",
    "dataset = dataset.merge(\n",
    "    arribos_por_ventana.rename(columns={'timestamp_rounded': 'timestamp'}),\n",
    "    on=['timestamp', 'id_estacion'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rellenar NaN con 0 (no hubo partidas/arribos)\n",
    "dataset['partidas'] = dataset['partidas'].fillna(0)\n",
    "dataset['arribos'] = dataset['arribos'].fillna(0)\n",
    "\n",
    "print(f\"‚úÖ Dataset combinado: {len(dataset)} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç AGREGANDO INFORMACI√ìN GEOGR√ÅFICA...\n",
      "‚úÖ Features geogr√°ficas y temporales agregadas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüìç AGREGANDO INFORMACI√ìN GEOGR√ÅFICA...\")\n",
    "\n",
    "dataset = dataset.merge(\n",
    "    todas_las_estaciones[['id_estacion', 'lat_estacion', 'long_estacion', 'nombre_estacion']],\n",
    "    on='id_estacion',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Agregar features temporales al dataset final\n",
    "dataset['hora'] = dataset['timestamp'].dt.hour\n",
    "dataset['dia_semana'] = dataset['timestamp'].dt.dayofweek  \n",
    "dataset['es_fin_de_semana'] = dataset['dia_semana'].isin([5, 6]).astype(int)\n",
    "dataset['mes'] = dataset['timestamp'].dt.month\n",
    "dataset['dia'] = dataset['timestamp'].dt.day\n",
    "\n",
    "print(f\"‚úÖ Features geogr√°ficas y temporales agregadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PASO 6: CREACI√ìN DE FEATURES DE PARTIDAS PASADAS PARA PREDICCI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è∞ CREANDO FEATURES DE PARTIDAS PASADAS...\n",
      "‚úÖ Features de lag creadas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n‚è∞ CREANDO FEATURES DE PARTIDAS PASADAS...\")\n",
    "\n",
    "# Ordenar por timestamp para crear las features de lag\n",
    "dataset = dataset.sort_values(['id_estacion', 'timestamp'])\n",
    "\n",
    "# Crear features de lag (partidas en ventanas anteriores)\n",
    "for lag in [1, 2, 3, 6]:  # 1, 2, 3 y 6 ventanas atr√°s (30, 60, 90, 180 mins)\n",
    "    dataset[f'partidas_lag_{lag}'] = dataset.groupby('id_estacion')['partidas'].shift(lag)\n",
    "\n",
    "# Features agregadas de partidas pasadas\n",
    "dataset['partidas_rolling_mean_3'] = dataset.groupby('id_estacion')['partidas'].rolling(window=3, min_periods=1).mean().values\n",
    "dataset['partidas_rolling_sum_6'] = dataset.groupby('id_estacion')['partidas'].rolling(window=6, min_periods=1).sum().values\n",
    "\n",
    "print(f\"‚úÖ Features de lag creadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 7: CREACI√ìN DE TARGETS FUTUROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ CREANDO TARGETS FUTUROS...\n",
      "‚úÖ Targets futuros creados\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüéØ CREANDO TARGETS FUTUROS...\")\n",
    "\n",
    "# Crear targets futuros (arribos en las pr√≥ximas ventanas)\n",
    "dataset['arribos_futuro_1'] = dataset.groupby('id_estacion')['arribos'].shift(-1)  # Pr√≥ximos 30 min\n",
    "dataset['arribos_futuro_2'] = dataset.groupby('id_estacion')['arribos'].shift(-2)  # Pr√≥ximos 60 min\n",
    "\n",
    "# Target principal: arribos en los pr√≥ximos 30 minutos\n",
    "dataset['target'] = dataset['arribos_futuro_1']\n",
    "\n",
    "print(f\"‚úÖ Targets futuros creados\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 8: FEATURE ENGINEERING ADICIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features de usuarios agregadas\n",
      "‚úÖ Features de ubicaci√≥n relativa agregadas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Agregar informaci√≥n de usuarios si est√° disponible\n",
    "if not usuarios_df.empty:\n",
    "    # Obtener estad√≠sticas de usuarios por viaje\n",
    "    user_stats = trips_df.groupby(['timestamp_rounded', 'id_estacion_origen'])['id_usuario'].agg(['count', 'nunique']).reset_index()\n",
    "    user_stats.columns = ['timestamp', 'id_estacion', 'total_viajes_usuarios', 'usuarios_unicos']\n",
    "    \n",
    "    dataset = dataset.merge(user_stats, on=['timestamp', 'id_estacion'], how='left')\n",
    "    dataset['total_viajes_usuarios'] = dataset['total_viajes_usuarios'].fillna(0)\n",
    "    dataset['usuarios_unicos'] = dataset['usuarios_unicos'].fillna(0)\n",
    "    \n",
    "    print(f\"‚úÖ Features de usuarios agregadas\")\n",
    "\n",
    "# Features de ubicaci√≥n relativa\n",
    "if not dataset.empty:\n",
    "    centro_lat = dataset['lat_estacion'].mean()\n",
    "    centro_long = dataset['long_estacion'].mean()\n",
    "    \n",
    "    dataset['distancia_al_centro'] = np.sqrt(\n",
    "        (dataset['lat_estacion'] - centro_lat)**2 + \n",
    "        (dataset['long_estacion'] - centro_long)**2\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Features de ubicaci√≥n relativa agregadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 9: PREPARACI√ìN DE DATOS PARA MODELADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéì PREPARANDO DATOS PARA MODELADO...\n",
      "üìä Dataset limpio: 4383032 registros\n",
      "üìä Features seleccionadas: 17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüéì PREPARANDO DATOS PARA MODELADO...\")\n",
    "\n",
    "# Seleccionar features para el modelo\n",
    "feature_columns = [\n",
    "    'hora', 'dia_semana', 'es_fin_de_semana', 'mes', 'dia',\n",
    "    'lat_estacion', 'long_estacion', 'distancia_al_centro',\n",
    "    'partidas', 'partidas_lag_1', 'partidas_lag_2', 'partidas_lag_3', 'partidas_lag_6',\n",
    "    'partidas_rolling_mean_3', 'partidas_rolling_sum_6'\n",
    "]\n",
    "\n",
    "# Agregar features de usuario si existen\n",
    "if 'total_viajes_usuarios' in dataset.columns:\n",
    "    feature_columns.extend(['total_viajes_usuarios', 'usuarios_unicos'])\n",
    "\n",
    "# Filtrar registros v√°lidos (sin NaN en target y con suficientes lags)\n",
    "dataset_clean = dataset.dropna(subset=['target'] + feature_columns)\n",
    "\n",
    "print(f\"üìä Dataset limpio: {len(dataset_clean)} registros\")\n",
    "print(f\"üìä Features seleccionadas: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà AN√ÅLISIS DE DISTRIBUCI√ìN DE TARGETS...\n",
      "Estad√≠sticas de arribos futuros:\n",
      "   - Media: 0.49\n",
      "   - Desviaci√≥n est√°ndar: 1.06\n",
      "   - M√°ximo: 31.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüìà AN√ÅLISIS DE DISTRIBUCI√ìN DE TARGETS...\")\n",
    "\n",
    "print(f\"Estad√≠sticas de arribos futuros:\")\n",
    "print(f\"   - Media: {dataset_clean['target'].mean():.2f}\")\n",
    "print(f\"   - Desviaci√≥n est√°ndar: {dataset_clean['target'].std():.2f}\")\n",
    "print(f\"   - M√°ximo: {dataset_clean['target'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 10: SPLIT TEMPORAL DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÇÔ∏è DIVIDIENDO DATOS TEMPORALMENTE...\n",
      "üìÖ Split temporal:\n",
      "   - Train: 3877688 registros (hasta 2024-08-02)\n",
      "   - Test: 505344 registros (desde 2024-08-02)\n",
      "‚úÖ Datos preparados para entrenamiento\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n‚úÇÔ∏è DIVIDIENDO DATOS TEMPORALMENTE...\")\n",
    "\n",
    "# Split temporal: √∫ltimas 2 semanas para test\n",
    "cutoff_date = dataset_clean['timestamp'].max() - timedelta(weeks=4)\n",
    "\n",
    "train_data = dataset_clean[dataset_clean['timestamp'] <= cutoff_date]\n",
    "test_data = dataset_clean[dataset_clean['timestamp'] > cutoff_date]\n",
    "\n",
    "print(f\"üìÖ Split temporal:\")\n",
    "print(f\"   - Train: {len(train_data)} registros (hasta {cutoff_date.strftime('%Y-%m-%d')})\")\n",
    "print(f\"   - Test: {len(test_data)} registros (desde {cutoff_date.strftime('%Y-%m-%d')})\")\n",
    "\n",
    "# Preparar X y y\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data['target']\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data['target']\n",
    "\n",
    "print(f\"‚úÖ Datos preparados para entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 11: ESCALADO DE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìè ESCALANDO FEATURES...\n",
      "‚úÖ Features escaladas\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìè ESCALANDO FEATURES...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Features escaladas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 12: ENTRENAMIENTO DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ ENTRENANDO MODELOS...\n",
      "\n",
      "üîÑ Entrenando Random Forest...\n",
      "   ‚úÖ Random Forest - MAE: 0.473, RMSE: 0.816, R2: 0.320\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "print(\"\\nü§ñ ENTRENANDO MODELOS...\")\n",
    "\n",
    "# Diccionario de modelos a probar\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100,max_depth=10, random_state=42),\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Entrenando {name}...\")\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    if name in ['Ridge Regression', 'Lasso Regression']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'model': model,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ {name} - MAE: {mae:.3f}, RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 13: EVALUACI√ìN Y COMPARACI√ìN DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüìä RESUMEN DE RESULTADOS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    name: {metric: results[name][metric] for metric in ['MAE', 'RMSE', 'R2']}\n",
    "    for name in results.keys()\n",
    "}).round(3)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# Encontrar el mejor modelo\n",
    "best_model_name = min(results.keys(), key=lambda x: results[x]['MAE'])\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
    "print(f\"   - MAE: {results[best_model_name]['MAE']:.3f}\")\n",
    "print(f\"   - RMSE: {results[best_model_name]['RMSE']:.3f}\")\n",
    "print(f\"   - R2: {results[best_model_name]['R2']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 14: AN√ÅLISIS DE IMPORTANCIA DE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüéØ AN√ÅLISIS DE IMPORTANCIA DE FEATURES...\")\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 features m√°s importantes:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'coefficient': abs(best_model.coef_)\n",
    "    }).sort_values('coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 features con mayores coeficientes:\")\n",
    "    print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 15: VALIDACI√ìN CON SERIES TEMPORALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüìà VALIDACI√ìN CON TIME SERIES SPLIT...\")\n",
    "\n",
    "# Usar TimeSeriesSplit para validaci√≥n m√°s robusta\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = []\n",
    "\n",
    "# Preparar datos ordenados por tiempo para CV\n",
    "train_data_sorted = train_data.sort_values('timestamp')\n",
    "X_cv = train_data_sorted[feature_columns]\n",
    "y_cv = train_data_sorted['target']\n",
    "\n",
    "# Validaci√≥n cruzada temporal\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_cv)):\n",
    "    X_train_cv, X_val_cv = X_cv.iloc[train_idx], X_cv.iloc[val_idx]\n",
    "    y_train_cv, y_val_cv = y_cv.iloc[train_idx], y_cv.iloc[val_idx]\n",
    "    \n",
    "    # Entrenar modelo del best performer\n",
    "    if best_model_name in ['Ridge Regression', 'Lasso Regression']:\n",
    "        X_train_cv_scaled = scaler.fit_transform(X_train_cv)\n",
    "        X_val_cv_scaled = scaler.transform(X_val_cv)\n",
    "        temp_model = type(best_model)(**best_model.get_params())\n",
    "        temp_model.fit(X_train_cv_scaled, y_train_cv)\n",
    "        y_pred_cv = temp_model.predict(X_val_cv_scaled)\n",
    "    else:\n",
    "        temp_model = type(best_model)(**best_model.get_params())\n",
    "        temp_model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred_cv = temp_model.predict(X_val_cv)\n",
    "    \n",
    "    mae_cv = mean_absolute_error(y_val_cv, y_pred_cv)\n",
    "    cv_scores.append(mae_cv)\n",
    "    \n",
    "    print(f\"   Fold {fold+1}: MAE = {mae_cv:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Validaci√≥n cruzada completada:\")\n",
    "print(f\"   - MAE promedio: {np.mean(cv_scores):.3f} (¬±{np.std(cv_scores):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 16: FUNCI√ìN DE PREDICCI√ìN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüîÆ CREANDO FUNCI√ìN DE PREDICCI√ìN...\")\n",
    "\n",
    "def predecir_arribos_futuro(timestamp_inicio, estaciones_partidas, modelo=best_model, \n",
    "                           scaler_obj=scaler, usar_escalado=best_model_name in ['Ridge Regression', 'Lasso Regression']):\n",
    "    \"\"\"\n",
    "    Predice arribos de bicicletas para todas las estaciones en los pr√≥ximos 30 minutos\n",
    "    \n",
    "    Parameters:\n",
    "    timestamp_inicio: datetime - momento desde el cual predecir\n",
    "    estaciones_partidas: dict - {id_estacion: cantidad_partidas} en √∫ltimos 30 min\n",
    "    \"\"\"\n",
    "    \n",
    "    predicciones = {}\n",
    "    \n",
    "    for id_estacion in todas_las_estaciones['id_estacion'].unique():\n",
    "        # Obtener info de la estaci√≥n\n",
    "        estacion_info = todas_las_estaciones[todas_las_estaciones['id_estacion'] == id_estacion].iloc[0]\n",
    "        \n",
    "        # Crear features para la predicci√≥n\n",
    "        features = {\n",
    "            'hora': timestamp_inicio.hour,\n",
    "            'dia_semana': timestamp_inicio.weekday(),\n",
    "            'es_fin_de_semana': 1 if timestamp_inicio.weekday() >= 5 else 0,\n",
    "            'mes': timestamp_inicio.month,\n",
    "            'dia': timestamp_inicio.day,\n",
    "            'lat_estacion': estacion_info['lat_estacion'],\n",
    "            'long_estacion': estacion_info['long_estacion'],\n",
    "            'distancia_al_centro': np.sqrt((estacion_info['lat_estacion'] - centro_lat)**2 + \n",
    "                                         (estacion_info['long_estacion'] - centro_long)**2),\n",
    "            'partidas': estaciones_partidas.get(id_estacion, 0),\n",
    "            'partidas_lag_1': 0,  # Simplificado para el ejemplo\n",
    "            'partidas_lag_2': 0,\n",
    "            'partidas_lag_3': 0,\n",
    "            'partidas_lag_6': 0,\n",
    "            'partidas_rolling_mean_3': estaciones_partidas.get(id_estacion, 0),\n",
    "            'partidas_rolling_sum_6': estaciones_partidas.get(id_estacion, 0)\n",
    "        }\n",
    "        \n",
    "        # Agregar features de usuario si existen\n",
    "        if 'total_viajes_usuarios' in feature_columns:\n",
    "            features['total_viajes_usuarios'] = estaciones_partidas.get(id_estacion, 0)\n",
    "            features['usuarios_unicos'] = min(estaciones_partidas.get(id_estacion, 0), 1)\n",
    "        \n",
    "        # Convertir a array para predicci√≥n\n",
    "        X_pred = np.array([[features[col] for col in feature_columns]])\n",
    "        \n",
    "        # Hacer predicci√≥n\n",
    "        if usar_escalado:\n",
    "            X_pred_scaled = scaler_obj.transform(X_pred)\n",
    "            pred = modelo.predict(X_pred_scaled)[0]\n",
    "        else:\n",
    "            pred = modelo.predict(X_pred)[0]\n",
    "        \n",
    "        predicciones[id_estacion] = max(0, round(pred))  # No puede ser negativo\n",
    "    \n",
    "    return predicciones\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de predicci√≥n creada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 17: EJEMPLO DE USO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüß™ EJEMPLO DE PREDICCI√ìN...\")\n",
    "\n",
    "# Crear un ejemplo de partidas por estaci√≥n en los √∫ltimos 30 min\n",
    "ejemplo_timestamp = datetime(2024, 8, 15, 14, 30)  # Ejemplo: 15 de agosto 2024, 14:30\n",
    "ejemplo_partidas = {\n",
    "    list(todas_las_estaciones['id_estacion'])[0]: 5,\n",
    "    list(todas_las_estaciones['id_estacion'])[1]: 3,\n",
    "    list(todas_las_estaciones['id_estacion'])[2]: 8,\n",
    "    # Resto con 0 partidas\n",
    "}\n",
    "\n",
    "# Completar con 0s para todas las estaciones\n",
    "for id_est in todas_las_estaciones['id_estacion']:\n",
    "    if id_est not in ejemplo_partidas:\n",
    "        ejemplo_partidas[id_est] = 0\n",
    "\n",
    "# Hacer predicci√≥n\n",
    "predicciones_ejemplo = predecir_arribos_futuro(ejemplo_timestamp, ejemplo_partidas)\n",
    "\n",
    "print(f\"üéØ Predicci√≥n para {ejemplo_timestamp.strftime('%Y-%m-%d %H:%M')}:\")\n",
    "print(\"Top 10 estaciones con m√°s arribos predichos:\")\n",
    "\n",
    "# Mostrar top 10 predicciones\n",
    "pred_sorted = sorted(predicciones_ejemplo.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (id_estacion, arribos) in enumerate(pred_sorted[:10]):\n",
    "    nombre = todas_las_estaciones[todas_las_estaciones['id_estacion']==id_estacion]['nombre_estacion'].iloc[0]\n",
    "    print(f\"   {i+1}. Estaci√≥n {id_estacion} ({nombre[:30]}...): {arribos} arribos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 18: AN√ÅLISIS DE PATRONES TEMPORALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüìä AN√ÅLISIS DE PATRONES TEMPORALES...\")\n",
    "\n",
    "# An√°lisis por hora del d√≠a\n",
    "patron_hora = dataset_clean.groupby('hora')['target'].mean()\n",
    "print(\"\\nPromedio de arribos por hora del d√≠a:\")\n",
    "for hora, arribos in patron_hora.items():\n",
    "    print(f\"   {hora:02d}:00 - {arribos:.1f} arribos promedio\")\n",
    "\n",
    "# An√°lisis por d√≠a de la semana\n",
    "dias_semana = ['Lunes', 'Martes', 'Mi√©rcoles', 'Jueves', 'Viernes', 'S√°bado', 'Domingo']\n",
    "patron_dia = dataset_clean.groupby('dia_semana')['target'].mean()\n",
    "print(\"\\nPromedio de arribos por d√≠a de la semana:\")\n",
    "for dia_num, arribos in patron_dia.items():\n",
    "    print(f\"   {dias_semana[dia_num]}: {arribos:.1f} arribos promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 19: M√âTRICAS DE NEGOCIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüíº M√âTRICAS DE NEGOCIO...\")\n",
    "\n",
    "# Calcular precisi√≥n en predicci√≥n de alta demanda (top 20% de arribos)\n",
    "threshold_alta_demanda = y_test.quantile(0.8)\n",
    "y_test_alta = (y_test >= threshold_alta_demanda).astype(int)\n",
    "y_pred_best = results[best_model_name]['predictions']\n",
    "y_pred_alta = (y_pred_best >= threshold_alta_demanda).astype(int)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision_alta = precision_score(y_test_alta, y_pred_alta, zero_division=0)\n",
    "recall_alta = recall_score(y_test_alta, y_pred_alta, zero_division=0)\n",
    "f1_alta = f1_score(y_test_alta, y_pred_alta, zero_division=0)\n",
    "\n",
    "print(f\"üìà Predicci√≥n de alta demanda (top 20%):\")\n",
    "print(f\"   - Precision: {precision_alta:.3f}\")\n",
    "print(f\"   - Recall: {recall_alta:.3f}\")\n",
    "print(f\"   - F1-Score: {f1_alta:.3f}\")\n",
    "\n",
    "# Error relativo promedio\n",
    "error_relativo = abs(y_test - y_pred_best) / (y_test + 1)  # +1 para evitar divisi√≥n por 0\n",
    "print(f\"\\nüìä Error relativo promedio: {error_relativo.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASO 20: CONCLUSIONES Y PR√ìXIMOS PASOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ RESUMEN FINAL DEL PROYECTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä DATASET PROCESADO:\")\n",
    "print(f\"   - Registros totales procesados: {len(dataset_clean):,}\")\n",
    "print(f\"   - Estaciones monitoreadas: {todas_las_estaciones['id_estacion'].nunique()}\")\n",
    "print(f\"   - Ventanas temporales de: {DELTA_T_MINUTES} minutos\")\n",
    "print(f\"   - Per√≠odo analizado: {trips_df['fecha_origen_recorrido'].min().strftime('%Y-%m-%d')} a {trips_df['fecha_origen_recorrido'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nü§ñ MEJOR MODELO: {best_model_name}\")\n",
    "print(f\"   - Error absoluto medio: {results[best_model_name]['MAE']:.2f} arribos\")\n",
    "print(f\"   - Error cuadr√°tico medio: {results[best_model_name]['RMSE']:.2f} arribos\")\n",
    "print(f\"   - R¬≤ Score: {results[best_model_name]['R2']:.3f}\")\n",
    "print(f\"   - Error relativo promedio: {error_relativo.mean():.1%}\")\n",
    "\n",
    "print(f\"\\nüéØ APLICACIONES PR√ÅCTICAS:\")\n",
    "print(f\"   - Redistribuci√≥n proactiva de bicicletas\")\n",
    "print(f\"   - Optimizaci√≥n de recursos de mantenimiento\")\n",
    "print(f\"   - Planificaci√≥n de capacidad por estaci√≥n\")\n",
    "print(f\"   - Alertas tempranas de alta/baja demanda\")\n",
    "\n",
    "print(f\"\\nüîÆ PREDICCI√ìN EJEMPLO:\")\n",
    "print(f\"   - Para el {ejemplo_timestamp.strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"   - Total arribos predichos: {sum(predicciones_ejemplo.values())} bicicletas\")\n",
    "print(f\"   - Estaci√≥n con mayor demanda predicha: {max(predicciones_ejemplo.values())} arribos\")\n",
    "\n",
    "print(f\"\\nüìà PR√ìXIMOS PASOS RECOMENDADOS:\")\n",
    "print(f\"   1. Implementar modelos m√°s sofisticados (LSTM, XGBoost)\")\n",
    "print(f\"   2. Incluir datos meteorol√≥gicos y eventos especiales\")\n",
    "print(f\"   3. Desarrollar API para predicciones en tiempo real\")\n",
    "print(f\"   4. Crear dashboard de monitoreo operativo\")\n",
    "print(f\"   5. Validar con datos de septiembre 2024 en adelante\")\n",
    "\n",
    "print(\"\\nüö¥ ¬°PROYECTO COMPLETADO EXITOSAMENTE! üö¥\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
