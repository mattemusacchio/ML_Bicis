"""
Utilidades comunes para el enfoque simplificado de predicci√≥n de arribos.
"""

import pandas as pd
import numpy as np
import pickle
from datetime import datetime
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import os

# Configuraci√≥n de la estaci√≥n objetivo
TARGET_STATION_ID = 14.0
TARGET_STATION_NAME = "014 - Pacifico"
TARGET_STATION_LAT = -34.577423
TARGET_STATION_LON = -58.426388

def load_trips_data(data_path="../../data/processed/trips_enriched.csv"):
    """
    Carga y prepara los datos de trips_enriched.csv
    
    Returns:
        pd.DataFrame: Dataset de trips con fechas parseadas
    """
    print("üîÑ Cargando datos de trips_enriched...")
    trips = pd.read_csv(data_path)
    
    # Convertir fechas
    trips['fecha_origen_recorrido'] = pd.to_datetime(trips['fecha_origen_recorrido'])
    trips['fecha_destino_recorrido'] = pd.to_datetime(trips['fecha_destino_recorrido'])
    
    print(f"‚úÖ Datos cargados: {trips.shape}")
    print(f"üìÖ Rango temporal: {trips['fecha_origen_recorrido'].min()} a {trips['fecha_origen_recorrido'].max()}")
    
    return trips

def calculate_distance(lat1, lon1, lat2, lon2):
    """
    Calcula la distancia euclidiana entre dos puntos geogr√°ficos.
    
    Args:
        lat1, lon1: Coordenadas del primer punto
        lat2, lon2: Coordenadas del segundo punto
    
    Returns:
        float: Distancia euclidiana
    """
    return np.sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2)

def get_nearby_stations(trips_df, target_lat, target_lon, radius=0.01, max_stations=20):
    """
    Encuentra estaciones cercanas a una ubicaci√≥n objetivo.
    
    Args:
        trips_df: DataFrame con datos de trips
        target_lat, target_lon: Coordenadas de la estaci√≥n objetivo
        radius: Radio de b√∫squeda (en grados, aprox 0.01 = ~1km)
        max_stations: M√°ximo n√∫mero de estaciones a retornar
    
    Returns:
        list: Lista de IDs de estaciones cercanas
    """
    print(f"üîç Buscando estaciones cercanas a ({target_lat}, {target_lon})...")
    
    # Obtener estaciones √∫nicas con coordenadas
    estaciones = trips_df[['id_estacion_origen', 'lat_estacion_origen', 'long_estacion_origen']].drop_duplicates()
    estaciones.columns = ['id_estacion', 'lat', 'lon']
    estaciones = estaciones.dropna()
    
    # Calcular distancias
    estaciones['distancia'] = calculate_distance(
        estaciones['lat'], estaciones['lon'], target_lat, target_lon
    )
    
    # Filtrar por radio y ordenar por distancia
    estaciones_cercanas = estaciones[estaciones['distancia'] <= radius].sort_values('distancia')
    
    if len(estaciones_cercanas) < 5:
        print(f"‚ö†Ô∏è  Solo {len(estaciones_cercanas)} estaciones en radio {radius}, expandiendo...")
        # Si hay muy pocas, tomar las m√°s cercanas
        estaciones_cercanas = estaciones.nsmallest(max_stations, 'distancia')
    
    estaciones_cercanas = estaciones_cercanas.head(max_stations)
    
    print(f"‚úÖ Encontradas {len(estaciones_cercanas)} estaciones cercanas")
    print(f"üìç Distancias: {estaciones_cercanas['distancia'].min():.4f} - {estaciones_cercanas['distancia'].max():.4f}")
    
    return estaciones_cercanas['id_estacion'].tolist()

def create_time_windows(trips_df, time_window_minutes=30):
    """
    Crea ventanas temporales para despachos y arribos.
    
    Args:
        trips_df: DataFrame con datos de trips
        time_window_minutes: Tama√±o de ventana en minutos
    
    Returns:
        pd.DataFrame: Dataset con ventanas temporales asignadas
    """
    print(f"‚è∞ Creando ventanas temporales de {time_window_minutes} minutos...")
    
    trips_df = trips_df.copy()
    
    # Ventana de despacho: el recorrido sirve como input para la ventana posterior
    trips_df['ventana_despacho'] = (
        trips_df['fecha_origen_recorrido'].dt.floor(f'{time_window_minutes}min') + 
        pd.Timedelta(minutes=time_window_minutes)
    )
    
    # Ventana de arribo: cae en la ventana actual
    trips_df['ventana_arribo'] = trips_df['fecha_destino_recorrido'].dt.floor(f'{time_window_minutes}min')
    
    print(f"‚úÖ Ventanas temporales creadas")
    
    return trips_df

def evaluate_model(model, X_train, y_train, X_val, y_val, model_name="Modelo"):
    """
    Eval√∫a un modelo y retorna m√©tricas.
    
    Args:
        model: Modelo entrenado
        X_train, y_train: Datos de entrenamiento
        X_val, y_val: Datos de validaci√≥n
        model_name: Nombre del modelo para logging
    
    Returns:
        dict: Diccionario con todas las m√©tricas
    """
    print(f"\nüìä Evaluando {model_name}...")
    
    # Predicciones
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)
    
    # M√©tricas
    metrics = {
        'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),
        'train_mae': mean_absolute_error(y_train, y_train_pred),
        'train_r2': r2_score(y_train, y_train_pred),
        'val_rmse': np.sqrt(mean_squared_error(y_val, y_val_pred)),
        'val_mae': mean_absolute_error(y_val, y_val_pred),
        'val_r2': r2_score(y_val, y_val_pred),
    }
    
    # Mostrar resultados
    print(f"\nüìà M√âTRICAS DE {model_name.upper()}")
    print("="*50)
    print(f"üöÇ ENTRENAMIENTO:")
    print(f"   RMSE: {metrics['train_rmse']:.4f}")
    print(f"   MAE:  {metrics['train_mae']:.4f}")
    print(f"   R¬≤:   {metrics['train_r2']:.4f}")
    
    print(f"\nüîç VALIDACI√ìN:")
    print(f"   RMSE: {metrics['val_rmse']:.4f}")
    print(f"   MAE:  {metrics['val_mae']:.4f}")
    print(f"   R¬≤:   {metrics['val_r2']:.4f}")
    
    # Indicador de overfitting
    r2_diff = metrics['train_r2'] - metrics['val_r2']
    if r2_diff > 0.1:
        print(f"\n‚ö†Ô∏è  Posible overfitting (diff R¬≤: {r2_diff:.3f})")
    elif r2_diff < 0:
        print(f"\n‚úÖ Buen ajuste (val R¬≤ > train R¬≤)")
    else:
        print(f"\n‚úÖ Buen balance train/val")
    
    return metrics, y_val_pred

def save_model(model, features, metrics, model_name, target_info=None):
    """
    Guarda un modelo entrenado con su metadata.
    
    Args:
        model: Modelo entrenado
        features: Lista de features utilizadas
        metrics: Diccionario con m√©tricas del modelo
        model_name: Nombre del archivo (sin extensi√≥n)
        target_info: Informaci√≥n adicional sobre el target
    """
    # Crear carpeta de modelos si no existe
    os.makedirs("../models", exist_ok=True)
    
    model_info = {
        'model': model,
        'features': features,
        'metrics': metrics,
        'train_timestamp': datetime.now().isoformat(),
        'target_station_id': TARGET_STATION_ID,
        'target_station_name': TARGET_STATION_NAME,
        'target_info': target_info
    }
    
    filepath = f"../models/{model_name}.pkl"
    
    with open(filepath, 'wb') as f:
        pickle.dump(model_info, f)
    
    print(f"\nüíæ Modelo guardado en: {filepath}")
    print(f"üìä R¬≤ validaci√≥n: {metrics['val_r2']:.4f}")

def load_model(model_path):
    """
    Carga un modelo previamente entrenado.
    
    Args:
        model_path: Ruta al archivo del modelo
    
    Returns:
        dict: Informaci√≥n completa del modelo
    """
    with open(model_path, 'rb') as f:
        model_info = pickle.load(f)
    
    print(f"üìÇ Modelo cargado: {model_path}")
    print(f"üéØ Estaci√≥n objetivo: {model_info['target_station_name']}")
    print(f"üìä R¬≤ validaci√≥n: {model_info['metrics']['val_r2']:.4f}")
    print(f"üìÖ Entrenado: {model_info['train_timestamp']}")
    
    return model_info

def print_feature_importance(model, features, top_n=15):
    """
    Muestra la importancia de features de un modelo XGBoost.
    
    Args:
        model: Modelo XGBoost entrenado
        features: Lista de nombres de features
        top_n: N√∫mero de features top a mostrar
    """
    if hasattr(model, 'feature_importances_'):
        importance_df = pd.DataFrame({
            'feature': features,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"\nüîù TOP {top_n} FEATURES M√ÅS IMPORTANTES:")
        print("="*50)
        for i, row in importance_df.head(top_n).iterrows():
            print(f"{row['feature']:<35} {row['importance']:.4f}")
        
        return importance_df
    else:
        print("\n‚ö†Ô∏è  El modelo no tiene feature_importances_")
        return None 