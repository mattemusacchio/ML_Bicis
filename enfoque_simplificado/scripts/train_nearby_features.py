"""
Entrenamiento con Features de Estaciones Cercanas
Entrena un modelo XGBoost usando despachos solo de estaciones geogr√°ficamente cercanas.
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import time
from feature_engineering import prepare_features_target
from utils import (
    TARGET_STATION_ID, TARGET_STATION_NAME,
    evaluate_model, save_model, print_feature_importance
)

def train_xgboost_nearby(X_train, y_train, X_val, y_val):
    """
    Entrena un modelo XGBoost optimizado para features de estaciones cercanas.
    
    Args:
        X_train, y_train: Datos de entrenamiento
        X_val, y_val: Datos de validaci√≥n
    
    Returns:
        modelo entrenado
    """
    print("\nüöÄ ENTRENANDO MODELO XGBOOST CON ESTACIONES CERCANAS")
    print("="*60)
    
    # Configuraci√≥n ajustada para menor n√∫mero de features
    xgb_params = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'n_estimators': 800,         # Menos √°rboles para evitar overfitting
        'max_depth': 6,              # Menor profundidad
        'learning_rate': 0.1,
        'subsample': 0.9,
        'colsample_bytree': 0.9,     # Mayor sampling de features
        'min_child_weight': 2,
        'reg_alpha': 0.05,           # Menos regularizaci√≥n L1
        'reg_lambda': 0.8,           # Menos regularizaci√≥n L2
        'random_state': 42,
        'n_jobs': -1,
        'verbosity': 1,
        'early_stopping_rounds': 80
    }
    
    print("‚öôÔ∏è  Configuraci√≥n del modelo:")
    for param, value in xgb_params.items():
        print(f"   {param}: {value}")
    
    # Crear el modelo
    model = xgb.XGBRegressor(**xgb_params)
    
    print(f"\nüîÑ Iniciando entrenamiento...")
    start_time = time.time()
    
    # Entrenar con early stopping
    model.fit(
        X_train, y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],
        verbose=False
    )
    
    training_time = time.time() - start_time
    print(f"‚úÖ Entrenamiento completado en {training_time:.2f} segundos")
    print(f"üå≥ √Årboles utilizados: {model.best_iteration + 1}")
    
    return model

def compare_with_global(metrics_nearby, global_model_path="../models/xgboost_global_features.pkl"):
    """
    Compara las m√©tricas del modelo de estaciones cercanas con el modelo global.
    
    Args:
        metrics_nearby: M√©tricas del modelo de estaciones cercanas
        global_model_path: Ruta al modelo global guardado
    """
    print(f"\nüÜö COMPARACI√ìN CON MODELO GLOBAL")
    print("="*50)
    
    try:
        import pickle
        with open(global_model_path, 'rb') as f:
            global_info = pickle.load(f)
        
        metrics_global = global_info['metrics']
        
        print(f"üìä M√âTRICAS COMPARATIVAS:")
        print(f"                      {'Global':<10} {'Cercanas':<10} {'Diferencia':<10}")
        print("="*50)
        
        for metric in ['val_r2', 'val_rmse', 'val_mae']:
            global_val = metrics_global[metric]
            nearby_val = metrics_nearby[metric]
            diff = nearby_val - global_val
            
            # Determinar si es mejor o peor
            if metric == 'val_r2':  # Para R¬≤, mayor es mejor
                better = "‚úÖ" if diff > 0 else "‚ùå"
            else:  # Para RMSE y MAE, menor es mejor
                better = "‚úÖ" if diff < 0 else "‚ùå"
            
            print(f"{metric:<15} {global_val:<10.4f} {nearby_val:<10.4f} {diff:>+9.4f} {better}")
        
        # An√°lisis general
        r2_diff = metrics_nearby['val_r2'] - metrics_global['val_r2']
        if r2_diff > 0.01:
            print(f"\nüéâ ¬°Modelo CERCANAS es MEJOR! (+{r2_diff:.3f} R¬≤)")
        elif r2_diff > -0.01:
            print(f"\nü§ù Ambos modelos tienen performance similar")
        else:
            print(f"\nüåê Modelo GLOBAL es mejor ({r2_diff:.3f} R¬≤)")
        
        # Analizar feature count
        global_features = len(global_info['features'])
        nearby_features = len(metrics_nearby.get('features', []))
        
        print(f"\nüìä COMPLEJIDAD:")
        print(f"   Global: {global_features} features")
        print(f"   Cercanas: {nearby_features} features")
        print(f"   Reducci√≥n: {((global_features - nearby_features) / global_features * 100):.1f}%")
        
    except FileNotFoundError:
        print("‚ö†Ô∏è  No se encontr√≥ el modelo global para comparar")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error al cargar modelo global: {e}")

def analyze_predictions(y_val, y_pred, model_name="Cercanas"):
    """
    Analiza las predicciones en detalle.
    
    Args:
        y_val: Valores reales
        y_pred: Predicciones del modelo
        model_name: Nombre del modelo para logging
    """
    print(f"\nüîç AN√ÅLISIS DETALLADO DE PREDICCIONES - {model_name.upper()}")
    print("="*60)
    
    # Estad√≠sticas b√°sicas
    print(f"üìä VALORES REALES:")
    print(f"   Min: {y_val.min()}")
    print(f"   Max: {y_val.max()}")
    print(f"   Media: {y_val.mean():.2f}")
    print(f"   Std: {y_val.std():.2f}")
    
    print(f"\nüìà PREDICCIONES:")
    print(f"   Min: {y_pred.min():.2f}")
    print(f"   Max: {y_pred.max():.2f}")
    print(f"   Media: {y_pred.mean():.2f}")
    print(f"   Std: {y_pred.std():.2f}")
    
    # An√°lisis de errores
    errors = y_pred - y_val
    abs_errors = np.abs(errors)
    
    print(f"\n‚ùå ERRORES:")
    print(f"   Error medio: {errors.mean():.2f}")
    print(f"   Error absoluto medio: {abs_errors.mean():.2f}")
    print(f"   Error m√°ximo: {abs_errors.max():.2f}")
    
    # Percentiles de errores
    print(f"\nüìè PERCENTILES DE ERRORES ABSOLUTOS:")
    percentiles = [50, 75, 90, 95, 99]
    for p in percentiles:
        print(f"   P{p}: {np.percentile(abs_errors, p):.2f}")

def main():
    """
    Pipeline principal de entrenamiento con estaciones cercanas.
    """
    print("üîç ENTRENAMIENTO CON ESTACIONES CERCANAS")
    print("="*70)
    print(f"üéØ Estaci√≥n objetivo: {TARGET_STATION_NAME} (ID: {TARGET_STATION_ID})")
    
    # 1. Cargar datos
    print("\nüìÇ Cargando datasets...")
    train_df = pd.read_csv('../data/train_nearby.csv')
    val_df = pd.read_csv('../data/val_nearby.csv')
    
    print(f"‚úÖ Train: {train_df.shape}")
    print(f"‚úÖ Val: {val_df.shape}")
    
    # Informaci√≥n sobre el target
    print(f"\nüéØ INFORMACI√ìN DEL TARGET:")
    print(f"   Train - Arribos promedio: {train_df['target_arribos'].mean():.2f}")
    print(f"   Train - Arribos std: {train_df['target_arribos'].std():.2f}")
    print(f"   Val - Arribos promedio: {val_df['target_arribos'].mean():.2f}")
    print(f"   Val - Arribos std: {val_df['target_arribos'].std():.2f}")
    
    # 2. Preparar features y target
    X_train, y_train, X_val, y_val, feature_names = prepare_features_target(train_df, val_df)
    
    # 3. Entrenar modelo
    model = train_xgboost_nearby(X_train, y_train, X_val, y_val)
    
    # 4. Evaluar modelo
    metrics, y_pred = evaluate_model(
        model, X_train, y_train, X_val, y_val, 
        model_name="XGBoost Cercanas"
    )
    
    # 5. An√°lisis detallado de predicciones
    analyze_predictions(y_val, y_pred, "Cercanas")
    
    # 6. Feature importance
    importance_df = print_feature_importance(model, feature_names, top_n=20)
    
    # 7. Comparar con modelo global
    metrics['features'] = feature_names  # Agregar features para comparaci√≥n
    compare_with_global(metrics)
    
    # 8. Guardar modelo
    target_info = {
        'approach': 'nearby_features',
        'description': 'Usa despachos solo de estaciones geogr√°ficamente cercanas',
        'num_features': len(feature_names),
        'train_period': f"{train_df['timestamp'].min()} to {train_df['timestamp'].max()}",
        'val_period': f"{val_df['timestamp'].min()} to {val_df['timestamp'].max()}"
    }
    
    save_model(
        model=model,
        features=feature_names,
        metrics=metrics,
        model_name="xgboost_nearby_features",
        target_info=target_info
    )
    
    # 9. Guardar feature importance
    if importance_df is not None:
        importance_df.to_csv('../data/feature_importance_nearby.csv', index=False)
        print(f"üíæ Feature importance guardada: ../data/feature_importance_nearby.csv")
    
    print("\nüéâ ENTRENAMIENTO COMPLETADO")
    print("="*50)
    print(f"üìä Resultado principal:")
    print(f"   üéØ R¬≤ validaci√≥n: {metrics['val_r2']:.4f}")
    print(f"   üìè RMSE validaci√≥n: {metrics['val_rmse']:.4f}")
    print(f"   üìê MAE validaci√≥n: {metrics['val_mae']:.4f}")
    
    if metrics['val_r2'] > 0.5:
        print("‚úÖ ¬°Excelente performance!")
    elif metrics['val_r2'] > 0.3:
        print("üëç Buena performance")
    elif metrics['val_r2'] > 0.1:
        print("‚ö†Ô∏è  Performance moderada")
    else:
        print("‚ùå Performance baja - revisar features o hiperpar√°metros")
    
    print(f"\nüîó Pr√≥ximo paso:")
    print("   Abrir notebook: ../notebooks/analisis_estacion_individual.ipynb")

if __name__ == "__main__":
    main() 